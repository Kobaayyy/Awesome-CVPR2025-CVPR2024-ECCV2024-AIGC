# Awesome-CVPR2025-AIGC[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A Collection of Papers and Codes for CVPR2025 AIGC

**整理汇总下今年CVPR AIGC相关的论文和代码，具体如下。**

**欢迎star，fork和PR~**

**Please feel free to star, fork or PR if helpful~**

# 相关整理

- [Awesome-ECCV2024-AIGC](https://github.com/Kobaayyy/Awesome-CVPR2024-ECCV2024-AIGC/blob/main/ECCV2024.md)
- [Awesome-AIGC-Research-Groups](https://github.com/Kobaayyy/Awesome-AIGC-Research-Groups)
- [Awesome-Low-Level-Vision-Research-Groups](https://github.com/Kobaayyy/Awesome-Low-Level-Vision-Research-Groups)
- [Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-CVPR2024-CVPR2021-CVPR2020-Low-Level-Vision)
- [Awesome-ECCV2020-Low-Level-Vision](https://github.com/Kobaayyy/Awesome-ECCV2020-Low-Level-Vision)
  
# **参考或转载请注明出处**

CVPR2025官网：[https://cvpr.thecvf.com/Conferences/2025](https://cvpr.thecvf.com/Conferences/2025)

CVPR接收论文列表：

CVPR完整论文库：

开会时间：2025月6月11日-2025月6月15日

论文接收公布时间：2025年2月27日

**【Contents】**

- [1.图像生成(Image Generation/Image Synthesis)](#1.图像生成)
- [2.图像编辑（Image Editing)](#2.图像编辑)
- [3.视频生成(Video Generation/Image Synthesis)](#3.视频生成)
- [4.视频编辑(Video Editing)](#4.视频编辑)
- [5.3D生成(3D Generation/3D Synthesis)](#5.3D生成)
- [6.3D编辑(3D Editing)](#6.3D编辑)
- [7.多模态大语言模型(Multi-Modal Large Language Model)](#7.大语言模型)
- [8.其他多任务(Others)](#8.其他)

<a name="1.图像生成"></a>

# 1.图像生成(Image Generation/Image Synthesis)

### CacheQuant: Comprehensively Accelerated Diffusion Models

- Paper: https://arxiv.org/abs/2503.01323
- Code: https://github.com/BienLuky/CacheQuant

### Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient

- Paper: https://arxiv.org/abs/2411.17787
- Code: https://github.com/czg1225/CoDe

### DiC: Rethinking Conv3x3 Designs in Diffusion Models

- Paper: https://arxiv.org/abs/2501.00603
- Code: https://github.com/YuchuanTian/DiC

### DreamText: High Fidelity Scene Text Synthesis

- Paper: https://arxiv.org/abs/2405.14701
- Code: https://github.com/CodeGoat24/DreamText

### Finding Local Diffusion Schrödinger Bridge using Kolmogorov-Arnold Network

- Paper: https://arxiv.org/abs/2502.19754
- Code: https://github.com/PerceptionComputingLab/LDSB
  
### Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models

- Paper: 
- Code: https://github.com/sccsok/CoprGuard
  
### Inversion Circle Interpolation: Diffusion-based Image Augmentation for Data-scarce Classification

- Paper: https://arxiv.org/abs/2408.16266
- Code: https://github.com/scuwyh2000/Diff-II
  
### Parallelized Autoregressive Visual Generation

- Paper: https://arxiv.org/abs/2412.15119
- Code: https://github.com/Epiphqny/PAR

### PatchDPO: Patch-level DPO for Finetuning-free Personalized Image Generation

- Paper: https://arxiv.org/abs/2412.03177
- Code: https://github.com/hqhQAQ/PatchDPO

### Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models

- Paper: https://arxiv.org/abs/2501.01423
- Code: https://github.com/hustvl/LightningDiT

### Rectified Diffusion Guidance for Conditional Generation

- Paper: https://arxiv.org/abs/2410.18737
- Code: https://github.com/thuxmf/recfg

### SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models

- Paper: https://arxiv.org/abs/2403.09055
- Code: https://github.com/ironjr/semantic-draw
  
### SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models

- Paper: https://arxiv.org/abs/2412.04852
- Code: https://github.com/taco-group/SleeperMark

### TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation

- Paper: https://arxiv.org/abs/2412.03069
- Code: https://github.com/ByteFlow-AI/TokenFlow
  
<a name="2.图像编辑"></a>

# 2.图像编辑(Image Editing)

### AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models

- Paper: https://arxiv.org/abs/2412.04146
- Code: https://github.com/Crayon-Shinchan/AnyDressing
  
### Attention Distillation: A Unified Approach to Visual Characteristics Transfer

- Paper: https://arxiv.org/abs/2502.20235
- Code: https://github.com/xugao97/AttentionDistillation

### Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing

- Paper: https://arxiv.org/abs/2411.16832
- Code: https://github.com/taco-group/FaceLock

### EmoEdit: Evoking Emotions through Image Manipulation

- Paper: https://arxiv.org/abs/2405.12661
- Code: https://github.com/JingyuanYY/EmoEdit
  
### K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs

- Paper: https://arxiv.org/abs/2502.18461
- Code: https://github.com/HVision-NKU/K-LoRA

### Recognition-Synergistic Scene Text Editing

- Paper: https://arxiv.org/abs/2502.18461
- Code: https://github.com/ZhengyaoFang/RS-STE
  
### StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements

- Paper: https://arxiv.org/abs/2412.08503
- Code: https://github.com/Westlake-AGI-Lab/StyleStudio
  
<a name="3.视频生成"></a>

# 3.视频生成(Video Generation/Video Synthesis)

### ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way

- Paper: https://arxiv.org/abs/2410.06241
- Code: https://github.com/Bujiazi/ByTheWay
  
### Identity-Preserving Text-to-Video Generation by Frequency Decomposition

- Paper: https://arxiv.org/abs/2411.17440
- Code: https://github.com/PKU-YuanGroup/ConsisID

### InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption

- Paper: https://arxiv.org/abs/2412.09283
- Code: https://github.com/NJU-PCALab/InstanceCap

### PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation

- Paper: https://arxiv.org/abs/2412.00596
- Code: https://github.com/pittisl/PhyT2V
  
### WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model

- Paper: https://arxiv.org/abs/2411.17459
- Code: https://github.com/PKU-YuanGroup/WF-VAE
  
<a name="4.视频编辑"></a>

# 4.视频编辑(Video Editing)

### Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models

- Paper: https://arxiv.org/abs/2407.15642
- Code: https://github.com/maxin-cn/Cinemo

### FADE: Frequency-Aware Diffusion Model Factorization for Video Editing

- Paper: 
- Code: https://github.com/EternalEvan/FADE
  
### Generative Inbetweening through Frame-wise Conditions-Driven Video Generation

- Paper: https://arxiv.org/abs/2412.11755
- Code: https://github.com/Tian-one/FCVG
  
### X-Dyna: Expressive Dynamic Human Image Animation

- Paper: https://arxiv.org/abs/2501.10021
- Code: https://github.com/bytedance/X-Dyna

<a name="5.3D生成"></a>

# 5.3D生成(3D Generation/3D Synthesis)

### Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play Deformation

- Paper: https://arxiv.org/abs/2411.16185
- Code: https://github.com/YuQiao0303/Fancy123

### Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass

- Paper: https://arxiv.org/abs/2501.13928
- Code: https://github.com/facebookresearch/fast3r
  
### GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation

- Paper: https://arxiv.org/abs/2406.06526
- Code: https://github.com/hzxie/GaussianCity

### LT3SD: Latent Trees for 3D Scene Diffusion

- Paper: https://arxiv.org/abs/2409.08215
- Code: https://github.com/quan-meng/lt3sd

### Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture

- Paper: https://arxiv.org/abs/2503.00495
- Code: https://github.com/XuanchenLi/TexTalk
  
### You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale

- Paper: https://arxiv.org/abs/2412.06699
- Code: https://github.com/baaivision/See3D
  
<a name="6.3D编辑"></a>

# 6.3D编辑(3D Editing)

### DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters

- Paper: https://arxiv.org/abs/2411.17423
- Code: https://github.com/yisuanwang/DRiVE

### FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video

- Paper: https://arxiv.org/abs/2411.15604
- Code: https://github.com/zjwfufu/FateAvatar

### Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters

- Paper: https://arxiv.org/abs/2411.18197
- Code: https://github.com/jasongzy/Make-It-Animatable
  
<a name="7.大语言模型"></a>

# 7.多模态大语言模型(Multi-Modal Large Language Models)

### Automated Generation of Challenging Multiple Choice Questions for Vision Language Model Evaluation

- Paper: https://arxiv.org/abs/2501.03225
- Code: https://github.com/yuhui-zh15/AutoConverter

### RAP-MLLM: Retrieval-Augmented Personalization for Multimodal Large Language Model

- Paper: https://arxiv.org/abs/2410.13360
- Code: https://github.com/Hoar012/RAP-MLLM

### SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model

- Paper: https://arxiv.org/abs/2412.01550
- Code: https://github.com/hq-King/SeqAfford

### ShowUI: One Vision-Language-Action Model for GUI Visual Agent

- Paper: https://arxiv.org/abs/2411.17465
- Code: https://github.com/showlab/ShowUI
  
<a name="8.其他"></a>

# 8.其他任务(Others)

### Continuous and Locomotive Crowd Behavior Generation

- Paper: 
- Code: https://github.com/InhwanBae/Crowd-Behavior-Generation

### Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis
  
- Paper: https://arxiv.org/abs/2412.15322
- Code: https://github.com/hkchengrex/MMAudio
  
<font color=red size=5>持续更新~</font>
